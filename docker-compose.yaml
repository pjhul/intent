services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: cohort
      POSTGRES_PASSWORD: cohort
      POSTGRES_DB: cohort
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U cohort"]
      interval: 5s
      timeout: 5s
      retries: 5

  kafka:
    image: apache/kafka:3.7.0
    hostname: kafka
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@localhost:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    ports:
      - "9092:9092"
      - "9094:9094"
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s

  kafka-init:
    image: apache/kafka:3.7.0
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        echo "Creating Kafka topics..."
        /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic events.raw --partitions 3 --replication-factor 1
        /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic cohort.definitions --partitions 1 --replication-factor 1 --config cleanup.policy=compact
        /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic cohort.membership --partitions 3 --replication-factor 1
        echo "Topics created successfully"

  clickhouse:
    image: clickhouse/clickhouse-server:24.1
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./deploy/clickhouse/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  flink-jobmanager:
    build:
      context: .
      dockerfile: deploy/flink/Dockerfile
    command: jobmanager
    ports:
      - "8081:8081"
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        state.backend: hashmap
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/overview"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s

  flink-taskmanager:
    build:
      context: .
      dockerfile: deploy/flink/Dockerfile
    command: taskmanager
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 2
        state.backend: hashmap
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints

  cohort-service:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      SERVER_HOST: "0.0.0.0"
      SERVER_PORT: "8080"
      POSTGRES_HOST: postgres
      POSTGRES_PORT: "5432"
      POSTGRES_USER: cohort
      POSTGRES_PASSWORD: cohort
      POSTGRES_DATABASE: cohort
      POSTGRES_SSL_MODE: disable
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: "9000"
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DATABASE: cohort
      KAFKA_BROKERS: kafka:9092
      KAFKA_EVENTS_TOPIC: events.raw
      KAFKA_COHORTS_TOPIC: cohort.definitions
      KAFKA_CHANGES_TOPIC: cohort.membership
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      FLINK_HOST: flink-jobmanager
      FLINK_PORT: "8081"

  inserter-service:
    build:
      context: .
      dockerfile: Dockerfile.inserter
    depends_on:
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      BATCH_SIZE: "1000"
      FLUSH_INTERVAL_MS: "5000ms"
      KAFKA_BROKERS: kafka:9092
      KAFKA_EVENTS_TOPIC: events.raw
      KAFKA_MEMBERSHIP_TOPIC: cohort.membership
      KAFKA_EVENTS_CONSUMER_GROUP: inserter-events
      KAFKA_MEMBERSHIP_CONSUMER_GROUP: inserter-membership
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: "9000"
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DATABASE: cohort

volumes:
  postgres_data:
  kafka_data:
  clickhouse_data:
  redis_data:
